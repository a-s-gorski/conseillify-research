{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Set, Optional\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from numpy.typing import NDArray, ArrayLike\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "import click\n",
    "import hdbscan\n",
    "import os\n",
    "import logging\n",
    "from random import sample\n",
    "import dill\n",
    "import time\n",
    "from sklearn.cluster import Birch\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# from ...features.build_features import load_pickle, save_pickle\n",
    "# from ...data.make_dataset import load_songs_encodings\n",
    "\n",
    "def missing_character(character):\n",
    "    return  \"\"\n",
    "\n",
    "def process_playlist_names(playlist_names: List[str]) -> List[List[str]]:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    printable = set(string.printable)\n",
    "    playlist_names = [''.join(filter(lambda sign: sign in printable, name)) for name in playlist_names]\n",
    "    playlist_names = [word_tokenize(sentence) for sentence in tqdm(playlist_names)]\n",
    "    playlist_names = [list(filter(lambda word: word.lower() not in stop_words, sentence)) for sentence in playlist_names]\n",
    "    return playlist_names\n",
    "\n",
    "\n",
    "def prepare_embedding_dict(model: Word2Vec) -> Dict[str, ArrayLike]:\n",
    "    embeddings = {word: np.array(embedding) for word, embedding in zip(model.wv.index_to_key, model.wv)}\n",
    "    return embeddings\n",
    "\n",
    "def embed_playlists(embeddings_dict: Dict[str, ArrayLike], playlist_names: List, playlist_len=10):\n",
    "    def embed_playlist(name):\n",
    "        if name in embeddings_dict:\n",
    "            return embeddings_dict[name]\n",
    "        else:\n",
    "            return \"\"\n",
    "    embedded_playlists = [np.array(list(map(embed_playlist, name[:playlist_len]))).flatten() for name in playlist_names]\n",
    "    max_name_len = max(list(map(len, embedded_playlists)))\n",
    "    embedded_playlists = np.array(list(map(lambda embedding: np.pad(embedding, (0, max_name_len - len(embedding)), 'constant', constant_values=(0,0)), embedded_playlists)))\n",
    "    return embedded_playlists\n",
    "\n",
    "def recommend_coldstart_cluster(clusterer: hdbscan.HDBSCAN, pca: PCA, playlist_name: str, embeddings_dict: Dict, max_name_len=100) -> int:\n",
    "    playlist_names = process_playlist_names([playlist_name,  ])[0]\n",
    "    playlist_names = playlist_names[:10]\n",
    "    playlist_embedding = np.array(list(map(lambda name: embeddings_dict[name], playlist_names))).flatten()\n",
    "    playlist_embedding = playlist_embedding[:max_name_len]\n",
    "    playlist_embedding = np.pad(playlist_embedding, (0, max_name_len - len(playlist_embedding)), 'constant', constant_values=(0,0)).reshape(1, -1)\n",
    "    playlist_embedding = pca.transform(playlist_embedding)\n",
    "    approx_labels, approx_probs = hdbscan.approximate_predict(clusterer, playlist_embedding)\n",
    "    probs = hdbscan.membership_vector(clusterer, playlist_embedding)\n",
    "    if not approx_labels:\n",
    "        return np.argmax(probs)\n",
    "    if approx_labels[0] == -1:\n",
    "        return np.argmax(probs)\n",
    "    return approx_labels[0]\n",
    "\n",
    "def select_n_from_cluster(clustered_tracks: Dict[int, Set[int]], cluster_id: int, n: int= 50):\n",
    "    tracks_set = clustered_tracks[cluster_id]\n",
    "    return sample(tracks_set, n)\n",
    "\n",
    "def cluster_tracks(labels: List[int], playlists: List[List[int]]) -> Dict[int, Set[int]]:\n",
    "    clustered_playlists = {}\n",
    "    for label, playlist in tqdm(zip(labels, playlists)):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        if label not in clustered_playlists:\n",
    "            clustered_playlists[label] = set()\n",
    "        for track in playlist:\n",
    "            if track == -1:\n",
    "                continue\n",
    "            clustered_playlists[label].add(track)\n",
    "    return clustered_playlists\n",
    "\n",
    "def missing_embedding():\n",
    "    return np.zeros(10)\n",
    "\n",
    "def missing_track():\n",
    "    return np.zeros(30)\n",
    "\n",
    "def missing_encoding():\n",
    "    return \"\"\n",
    "\n",
    "def prepare_playlist(embeddings_dict, pca, user_playlist) -> NDArray:\n",
    "    print(user_playlist)\n",
    "    processed_playlist = process_playlist_names(user_playlist)\n",
    "    print(processed_playlist)\n",
    "    e_user_playlist = embed_playlists(embeddings_dict, processed_playlist)\n",
    "    e_user_playlist = list(map(lambda p: p[:100], e_user_playlist))\n",
    "    e_user_playlist = np.array(list(map(lambda p: np.pad(p, (0, 100-len(p)), 'constant', constant_values=(0,0)), e_user_playlist)))\n",
    "    print(e_user_playlist.shape)\n",
    "    rd_user_playlist = pca.transform(e_user_playlist)\n",
    "    print(rd_user_playlist.shape)\n",
    "    return rd_user_playlist\n",
    "\n",
    "\n",
    "def empty_track():\n",
    "    return -1\n",
    "\n",
    "def load_songs_encodings(input_path: str) -> Dict[str, int]:\n",
    "    songs_encodings_df = pd.read_csv(input_path, index_col=False)\n",
    "    songs_encodings = defaultdict(empty_track)\n",
    "    for track_uri, track_encoding in zip(songs_encodings_df.track_uris, songs_encodings_df.track_encoding):\n",
    "        songs_encodings[track_uri] = track_encoding\n",
    "    return songs_encodings\n",
    "\n",
    "\n",
    "# @click.command()\n",
    "# @click.argument('input_filepath', type=click.Path(exists=True))\n",
    "# @click.argument('model_filepath', type=click.Path(exists=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filepath = \"/home/adamgorski/Desktop/inzynierka/conseillify/conseillify-research/data/processed\"\n",
    "model_filepath = \"/home/adamgorski/Desktop/inzynierka/conseillify/conseillify-research/models/coldstart_birch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:42<00:00, 23383.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1000000, 100)\n"
     ]
    }
   ],
   "source": [
    "data_processing_start = time.time()\n",
    "logging.info(\"Loading playlists data\")\n",
    "playlist_names = pd.read_csv(os.path.join(input_filepath, \"playlist_names.csv\"))[\"0\"].to_numpy(dtype=str)\n",
    "playlists = pd.read_csv(os.path.join(input_filepath, \"playlists.csv\")).to_numpy()\n",
    "\n",
    "logging.info(\"Loading songs encodings\")\n",
    "songs_encodings = load_songs_encodings(os.path.join(input_filepath, \"songs_encodings.csv\"))\n",
    "songs_encodings = {track_id: track_uri for track_uri, track_id in songs_encodings.items()}\n",
    "logging.info(\"Processing playlist info\")\n",
    "playlist_names = process_playlist_names(playlist_names)\n",
    "\n",
    "logging.info(\"Training word2vec model\")\n",
    "model = gensim.models.Word2Vec(playlist_names, min_count=1, vector_size=10, window=5)\n",
    "\n",
    "logging.info(\"Building embeddings\")\n",
    "embeddings_dict = prepare_embedding_dict(model)\n",
    "embedded_playlists = embed_playlists(embeddings_dict, playlist_names)\n",
    "logging.info(f\"Data processing execution time {time.time() - data_processing_start}\")\n",
    "print(type(embedded_playlists))\n",
    "print(embedded_playlists.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "reduced_pn = pca.fit_transform(embedded_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2270 4075 4681 2156 4681 2639 4681 3591  961 4681 4681 3020 4681 1615\n",
      " 1946 4681  585 3497 3323 2745 4681 4171 4681 4681 2377 4502 4681 4681\n",
      " 4681 4681 1008 2481 1119 4681 4681 4681 4681 3647 3020 4681 2133 4681\n",
      " 4681 4681 4681 2801 4681 4681 4681 4681 4179 3311 1904 4681  168 4681\n",
      " 4681 4681   -1   28 4681 2885 4681 4681 4681 1788 4681 4681  452 2970\n",
      " 3709 1526 2246 4681 4681 4681 2270 4681  430  290 3337 1506  799 3871\n",
      " 4681 4681 4566 4681 4681 2147 4681 4681 1717 2055 4681 4681 3349 2147\n",
      " 4167 2451]\n",
      "4683\n"
     ]
    }
   ],
   "source": [
    "clusterer = hdbscan.HDBSCAN(cluster_selection_epsilon=1)\n",
    "clusterer.fit(reduced_pn)\n",
    "print(clusterer.labels_[:100])\n",
    "n_labels = len(set(clusterer.labels_))\n",
    "print(n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "brc= Birch(n_clusters=n_labels)\n",
    "labels = brc.fit_predict(reduced_pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_labels(labels: List[int], playlists: NDArray, songs_encodings: Dict[int, str]) -> Dict[int, Set[str]]:\n",
    "    clustered_tracks = {}\n",
    "    for playlist_index, (cluster_index, playlist) in tqdm(enumerate(zip(labels, playlists)), total=playlists.shape[0]):\n",
    "        if not cluster_index in clustered_tracks:\n",
    "            clustered_tracks[cluster_index] = set()\n",
    "        for track in playlist:\n",
    "            if track != -1:\n",
    "                clustered_tracks[cluster_index].add(songs_encodings[track])\n",
    "    return clustered_tracks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [01:27<00:00, 11471.46it/s]\n"
     ]
    }
   ],
   "source": [
    "clustered_tracks = cluster_labels(labels, playlists, songs_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37132\n"
     ]
    }
   ],
   "source": [
    "print(len(clustered_tracks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rock and roll']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4466.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rock', 'roll']]\n",
      "(1, 100)\n",
      "(1, 10)\n",
      "[3878]\n"
     ]
    }
   ],
   "source": [
    "user_playlist = [\"rock and roll\"]\n",
    "rd_user_playlist = prepare_playlist(embeddings_dict, pca, user_playlist)\n",
    "print(brc.predict(rd_user_playlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_n_tracks(brc: Birch, clustered_tracks: Dict[int, Set], processed_playlist: List[float], n_recommendations: Optional[int]=100):\n",
    "    cluster_id = brc.predict(processed_playlist)[0]\n",
    "    return random.sample(tuple(clustered_tracks[cluster_id]), n_recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "recs = recommend_n_tracks(brc, clustered_tracks, rd_user_playlist)\n",
    "print(len(recs))\n",
    "print(len(set(recs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from typing import Any\n",
    "def save_pickle(object: Any, output_path: str):\n",
    "    with open(output_path, 'wb+') as file:\n",
    "        pickle.dump(object, file)\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "save_pickle(brc, os.path.join(model_filepath, \"brc.pkl\"))\n",
    "save_pickle(pca, os.path.join(model_filepath, \"pca.pkl\"))\n",
    "save_pickle(embeddings_dict, os.path.join(model_filepath, \"embeddings_dict.pkl\"))\n",
    "save_pickle(clustered_tracks, os.path.join(model_filepath, \"clustered_tracks.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "520163f1cd47558842bbeb3d8018629964a373bd06cc7061edb7366f9d3fb8a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
